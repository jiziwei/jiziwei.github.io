<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Ziwei Ji</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Ziwei Ji</h1>
</div>
<p>I am a research scientist at <a href="https://research.google/" target=&ldquo;blank&rdquo;>Google Research</a>, New
York City.
Previously, I received my PhD degree from
<a href="https://cs.illinois.edu/" target=&ldquo;blank&rdquo;>University of Illinois Urbana-Champaign</a>, advised by
<a href="http://mjt.cs.illinois.edu/" target=&ldquo;blank&rdquo;>Matus Telgarsky</a>.
I obtained my bachelor's degree in Computer Science from the ACM
class at Shanghai Jiao Tong University.
I am interested in machine learning and optimization, particularly in deep
learning theory.
</p>
<p>Email: ziweiji (at) google (dot) com
</p>
<p><a href="cv.pdf" target=&ldquo;blank&rdquo;>CV.</a>
<a href="https://scholar.google.com/citations?user=3l_6H5sAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar.</a>
<a href="proposal.pdf" target=&ldquo;blank&rdquo;></a>
<a href="thesis.pdf" target=&ldquo;blank&rdquo;></a>
</p>
<h2>Publications</h2>
<h3>Manuscripts</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.04598" target=&ldquo;blank&rdquo;>Reproducibility in Optimization: Theoretical Framework and Limits</a> <br />
Kwangjun Ahn, Prateek Jain, Ziwei Ji, Satyen Kale, Praneeth Netrapalli, Gil I. Shamir. <br />
</p>
</li>
</ul>
<h3>Conference publications</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2201.13419" target=&ldquo;blank&rdquo;>Agnostic Learnability of Halfspaces via Logistic Loss</a> <br />
Ziwei Ji, Kwangjun Ahn, Pranjal Awasthi, Satyen Kale, Stefani Karp. <br />
ICML 2022.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.11280" target=&ldquo;blank&rdquo;>Actor-critic is implicitly biased towards high entropy optimal policies</a> <br />
Yuzheng Hu, Ziwei Ji, Matus Telgarsky. <br />
ICLR 2022.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2106.05932" target=&ldquo;blank&rdquo;>Early-stopped neural networks are consistent</a> <br />
Ziwei Ji, Justin D. Li, Matus Telgarsky. <br />
NeurIPS 2021, Spotlight.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2107.00595" target=&ldquo;blank&rdquo;>Fast Margin Maximization via Dual Acceleration</a> <br />
Ziwei Ji, Nathan Srebro, Matus Telgarsky. <br />
ICML 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2104.05641" target=&ldquo;blank&rdquo;>Generalization bounds via distillation</a> <br />
Daniel Hsu, Ziwei Ji, Matus Telgarsky, Lan Wang. <br />
ICLR 2021, Spotlight.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1906.04540" target=&ldquo;blank&rdquo;>Characterizing the implicit bias via a primal-dual analysis</a> <br />
Ziwei Ji, Matus Telgarsky. <br />
ALT 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.06657" target=&ldquo;blank&rdquo;>Directional convergence and alignment in deep learning</a> <br />
Ziwei Ji, Matus Telgarsky. <br />
NeurIPS 2020, Spotlight.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.11226" target=&ldquo;blank&rdquo;>Gradient descent follows the regularization path for general losses</a> <br />
Ziwei Ji, Miroslav Dudík, Robert E. Schapire, Matus Telgarsky. <br />
COLT 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1909.12292" target=&ldquo;blank&rdquo;>Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks</a> <br />
Ziwei Ji, Matus Telgarsky. <br />
ICLR 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1910.06956" target=&ldquo;blank&rdquo;>Neural tangent kernels, transportation mappings, and universal approximation</a> <br />
Ziwei Ji, Matus Telgarsky, Ruicheng Xian. <br />
ICLR 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1803.07300" target=&ldquo;blank&rdquo;>Risk and parameter convergence of logistic regression</a> <br />
Ziwei Ji, Matus Telgarsky. <br />
In COLT 2019 under the name &ldquo;The implicit bias of gradient descent on nonseparable data&rdquo;.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.02032" target=&ldquo;blank&rdquo;>Gradient descent aligns the layers of deep linear networks</a> <br />
Ziwei Ji, Matus Telgarsky. <br />
ICLR 2019.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1711.02211" target=&ldquo;blank&rdquo;>Social Welfare and Profit Maximization from Revealed Preferences</a> <br />
Ziwei Ji, Ruta Mehta, Matus Telgarsky. <br />
WINE 2018.
</p>
</li>
</ul>
<h2>Talks</h2>
<ul>
<li><p><b>The dual of the margin: improved analyses and rates for gradient descent’s implicit bias</b> <br />
One World Seminar Series on the Mathematics of Machine Learning, December 2020.
</p>
</li>
</ul>
<ul>
<li><p><b>Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks</b> <br />
Illinois Institute for Data Science and Dynamical Systems (IDS2) Seminar Series, April 2020; <br />
14th Annual Machine Learning Symposium, The New York Academy of Sciences, March 2020; <br />
15th CSL Student Conference, February 2020.
</p>
</li>
</ul>
<h2>Teaching</h2>
<ul>
<li><p>Teaching assistant for UIUC CS 598 Deep Learning Theory (Fall 2020, Fall 2021).
</p>
</li>
</ul>
<ul>
<li><p>Teaching assistant for UIUC CS 446 Machine Learning (Spring 2019).
</p>
</li>
</ul>
<h2>Service</h2>
<p>Reviewer for NeurIPS, ICLR, COLT, ICML, EC, ITCS, IEEE Transactions on Information Theory.
</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-06-11 13:11:10 EDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
